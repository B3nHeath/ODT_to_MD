

## Conjoint Analysis Applications in Health—a Checklist: A Report of the ISPOR Good Research Practices for Conjoint Analysis Task Force

**Synopsis:**

A review which identifies many of the good practices for Conjoint analysis, and the various designs that can be introduced. Includes methods of profile randomization, appropriate number of tasks, the need to ensure tasks are not too complicated, and improve capacity by providing an example version. Also mentions the need to check for demographic differences between responders and non-responders

## By John Bridges et al. (2011)

Need to consider full profiles, consider concerns over whether patients can takn in full profile, might partial profile be better -pg.407



Number of different options for a specific variable needs to be controlled, 3 or 4 is good in health care according to this study -pg.406



If there is concern over the number of attributes, then researchers can present full profiles but keep some of the attributes the same in the presented options -pg.407



“In some studies, subjects may be presented with a set of many alternative profiles and asked to order or rank the profiles from most preferred to least preferred. In this type of study, subjects often complete only one task. In other studies, profiles are grouped into sets, and respondents are asked to choose among the alternatives in each set.” -pg.407



Inclusion of an opt-out or status quo option. Needs to be considered careful, opt-out option leads to censoring of data



“A design is orthogonal if all attribute levels vary independently, and thus are not correlated. A design is balanced when each level of an attribute occurs the same number of times. A design is efficient when it has the smallest variance matrix. Efficient designs are orthogonal and balanced if the underlying statistical model assumes linearity. For nonlinear statistical models (e.g., the multinomial logit model), orthogonality and level balance may not result in the most efficient design. Design algorithms seeking to maximize efficiency can be used.” -pg.407



“Theoretically efficient designs may have undesirable empirical properties. For example, the variance of the design depends on the actual parameter values. Most design programs assume equal preference weights in calculating efficiency” -pg.407



Need to worry about implausible attribute combinations, potentially goes against orthogonality -pg.407



“Researchers have offered several alternative approaches for conjoint studies, including D-optimal and near-D-optimal designs [45,46], utility-imbalanced designs [47], cyclical designs [48], and random designs [49]” -pg.408



“Potential criteria for evaluating designs include the following: ● Efficiency score, ● Correlations among attribute levels, ● Correlations among attribute-level differences, ● Level balance, ● Number of overlapping attributes, ● Restrictions on implausible combinations, ● Cognitive difficulty.” -pg.408



“Experimental-design programs such as SAS (Cary, NC), SPSS (Chicago, IL), and Sawtooth Software (Sequim, WA) typically generate a number of design diagnostics to assist in evaluating designs, and at least one website offers a service to measure the relative efficiency of any design [51]” -pg.408



“There is still much debate in health care research as to the appropriate number of conjoint-analysis tasks a respondent can complete, but it is good practice to include 8 to 16 conjointanalysis tasks. Still, some conjoint analysis practitioners advocate that respondents can complete up to 32 tasks” -pg.408



When getting participants to complete the task, providing them information as to the importance of the task can improve the quality of responses you get, preventing boredom and information overload from the task. At same time, too much information can lead to yeasaying, strategic voting, and information overload -pg.408



Introducing attributes and levels prior to the task is also good practice, participant comprehension can be tested by asking a simple question around the attributes. -pg.408



Some researchers use ‘cheap talk’, statements intended to motivate participants, though effect of this is unclear -pg.408



To prevent people choosing the opt-out option merely to avoid answering hypothetical, can include opt-out as an option against the chosen option from the task in the next window -pg.408



“The number of attributes and levels may induce measurement error as well [56]. Work by Kjaer and colleagues [57] suggested that respondents can show differential sensitivity to price, depending on where the cost attribute occurs in the profile. Varying the order of attributes may be prudent. Randomizing the order of tasks is good practice.” -pg.409



Suggest detailed pretests to check the survey design and ensure that it is functional



“Historically, researchers commonly applied rules of thumb, based on the number of attribute levels, to estimate sample size [58]. Orme [58] recommended sample sizes of at least 300 with a minimum of 200 respondents per group for subgroup analysis. Marshall et al. [59] reported that the mean sample size for conjoint-analysis studies in health care published between 2005 and 2008 was 259, with nearly 40% of the sample sizes in the range of 100 to 300 respondents.” -pg.409



“As part of the statistical analysis, good research practices require an assessment of the respondent characteristics, the quality of the responses, and a description and justification for the model estimation methods” -pg.409



Also want to looks at the differences in demographics between responders and non-responders to check that there are no worrying, identifiable patterns -pg.410



Can check response quality through question repetition, provision of a clearly better option, or checking transitivity of responses -pg.410









https://www.sciencedirect.com/science/article/pii/S1098301510000835?via%3Dihub

